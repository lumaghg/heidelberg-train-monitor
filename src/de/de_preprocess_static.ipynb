{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08139850",
   "metadata": {},
   "source": [
    "# Preprocess static data from DB Timetables API\n",
    "\n",
    "- script runs hourly\n",
    "- to be able to know about every train that is currently on the way, we need data for current_time +- time of the longest trip between stations without stopping, which is 4h (FFM-Berlin Sprinter). Data should last for 1 hour before updating. Therefore we need to request current hour, the 4 hours before that and the 4 hours after that = 9 hours. \n",
    "Steps:\n",
    "- delete data from timetable that is for an hour before current hour - 4. \n",
    "- request the timetable from every relevant station, and parse the xmls into a timetable while filtering for Fernverkehr (category in [ICE, IC, EC, NJ ...]). Note which date/hour was used to make the request. \n",
    "- artificially add departure and arrival dates where missing\n",
    "- sort by I. tripid 2. departure time / stop order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d28c235",
   "metadata": {},
   "source": [
    "### helper functions for handling db dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea6a6813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250810\n",
      "2508101222\n",
      "2025-08-10\n",
      "2025-08-10 12:22:00\n",
      "('250810', '12')\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "STOPTIMES_PATH = 'stoptimes.csv'\n",
    "STATIONS_PATH = './static/stations.csv'\n",
    "\n",
    "# chatgpt generiert lol\n",
    "import datetime\n",
    "def dateToDBDate(date: datetime.date) -> str:\n",
    "    \"\"\"\n",
    "    Wandelt ein datetime.date Objekt in einen DB-Date-String \"YYMMDD\" um.\n",
    "    \"\"\"\n",
    "    return date.strftime(\"%y%m%d\")\n",
    "\n",
    "\n",
    "def datetimeToDBDatetime(dt: datetime.datetime) -> str:\n",
    "    \"\"\"\n",
    "    Wandelt ein datetime.datetime Objekt in einen DB-Datetime-String \"YYMMDDHHMM\" um.\n",
    "    \"\"\"\n",
    "    return dt.strftime(\"%y%m%d%H%M\")\n",
    "\n",
    "\n",
    "def DBDatetimeToDatetime(dbDate: str) -> datetime.datetime:\n",
    "    \"\"\"\n",
    "    Wandelt einen DB-Datetime-String \"YYMMDDHHMM\" in ein datetime.datetime Objekt um.\n",
    "    \"\"\"\n",
    "    return datetime.datetime.strptime(dbDate, \"%y%m%d%H%M\")\n",
    "\n",
    "\n",
    "def DBDateToDate(dbDate: str) -> datetime.date:\n",
    "    \"\"\"\n",
    "    Wandelt einen DB-Date-String \"YYMMDD\" in ein datetime.date Objekt um.\n",
    "    \"\"\"\n",
    "    return datetime.datetime.strptime(dbDate, \"%y%m%d\").date()\n",
    "\n",
    "def datetimeToDBDateAndHourTuple(dt: datetime.datetime):\n",
    "    date = dt.strftime(\"%y%m%d\")\n",
    "    hour = dt.strftime(\"%H\")\n",
    "    return (date, hour)\n",
    "    \n",
    "       \n",
    "print(dateToDBDate(datetime.date(2025, 8, 10)))\n",
    "print(datetimeToDBDatetime(datetime.datetime(2025, 8, 10, 12, 22)))\n",
    "print(DBDateToDate(\"250810\"))\n",
    "print(DBDatetimeToDatetime(\"2508101222\"))\n",
    "print(datetimeToDBDateAndHourTuple(datetime.datetime(2025, 8, 10, 12, 22)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c95349d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('250822', '05'), ('250822', '06'), ('250822', '07'), ('250822', '08'), ('250822', '09'), ('250822', '10'), ('250822', '11'), ('250822', '12'), ('250822', '13')]\n"
     ]
    }
   ],
   "source": [
    "# calculate which times need to be requested\n",
    "\n",
    "date_hour_tuples_to_request = []\n",
    "\n",
    "current_datetime = datetime.datetime.today()\n",
    "\n",
    "for hour_offset in range(-4,5):\n",
    "    datetime_to_request = current_datetime + datetime.timedelta(hours=hour_offset)\n",
    "    date_hour_tuple = datetimeToDBDateAndHourTuple(datetime_to_request)\n",
    "    date_hour_tuples_to_request.append(datetimeToDBDateAndHourTuple(datetime_to_request))\n",
    "\n",
    "\n",
    "print(date_hour_tuples_to_request)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "467f89b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      station_name station_uic\n",
      "0   Hamburg-Altona     8002553\n",
      "1  Hamburg Dammtor     8002548\n",
      "2      Hamburg Hbf     8002549\n",
      "3  Hamburg-Harburg     8000147\n",
      "4         Lüneburg     8000238\n",
      "5           Uelzen     8000168\n",
      "6            Celle     8000064\n",
      "7     Hannover Hbf     8000152\n"
     ]
    }
   ],
   "source": [
    "# load stations that need to be requested\n",
    "df_stations = pd.read_csv(STATIONS_PATH, dtype=str).dropna(how='all')\n",
    "print(df_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34c371ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting Hamburg-Altona\n",
      "fetching 250822 / 05:00 / Hamburg-Altona\n",
      "fetching 250822 / 06:00 / Hamburg-Altona\n",
      "fetching 250822 / 07:00 / Hamburg-Altona\n",
      "fetching 250822 / 08:00 / Hamburg-Altona\n",
      "fetching 250822 / 09:00 / Hamburg-Altona\n",
      "fetching 250822 / 10:00 / Hamburg-Altona\n",
      "fetching 250822 / 11:00 / Hamburg-Altona\n",
      "fetching 250822 / 12:00 / Hamburg-Altona\n",
      "fetching 250822 / 13:00 / Hamburg-Altona\n",
      "requesting Hamburg Dammtor\n",
      "fetching 250822 / 05:00 / Hamburg Dammtor\n",
      "fetching 250822 / 06:00 / Hamburg Dammtor\n",
      "fetching 250822 / 07:00 / Hamburg Dammtor\n",
      "fetching 250822 / 08:00 / Hamburg Dammtor\n",
      "fetching 250822 / 09:00 / Hamburg Dammtor\n",
      "fetching 250822 / 10:00 / Hamburg Dammtor\n",
      "fetching 250822 / 11:00 / Hamburg Dammtor\n",
      "fetching 250822 / 12:00 / Hamburg Dammtor\n",
      "fetching 250822 / 13:00 / Hamburg Dammtor\n",
      "requesting Hamburg Hbf\n",
      "fetching 250822 / 05:00 / Hamburg Hbf\n",
      "fetching 250822 / 06:00 / Hamburg Hbf\n",
      "fetching 250822 / 07:00 / Hamburg Hbf\n",
      "fetching 250822 / 08:00 / Hamburg Hbf\n",
      "fetching 250822 / 09:00 / Hamburg Hbf\n",
      "fetching 250822 / 10:00 / Hamburg Hbf\n",
      "fetching 250822 / 11:00 / Hamburg Hbf\n",
      "fetching 250822 / 12:00 / Hamburg Hbf\n",
      "fetching 250822 / 13:00 / Hamburg Hbf\n",
      "requesting Hamburg-Harburg\n",
      "fetching 250822 / 05:00 / Hamburg-Harburg\n",
      "fetching 250822 / 06:00 / Hamburg-Harburg\n",
      "fetching 250822 / 07:00 / Hamburg-Harburg\n",
      "fetching 250822 / 08:00 / Hamburg-Harburg\n",
      "fetching 250822 / 09:00 / Hamburg-Harburg\n",
      "fetching 250822 / 10:00 / Hamburg-Harburg\n",
      "fetching 250822 / 11:00 / Hamburg-Harburg\n",
      "fetching 250822 / 12:00 / Hamburg-Harburg\n",
      "fetching 250822 / 13:00 / Hamburg-Harburg\n",
      "requesting Lüneburg\n",
      "fetching 250822 / 05:00 / Lüneburg\n",
      "fetching 250822 / 06:00 / Lüneburg\n",
      "fetching 250822 / 07:00 / Lüneburg\n",
      "fetching 250822 / 08:00 / Lüneburg\n",
      "fetching 250822 / 09:00 / Lüneburg\n",
      "fetching 250822 / 10:00 / Lüneburg\n",
      "fetching 250822 / 11:00 / Lüneburg\n",
      "fetching 250822 / 12:00 / Lüneburg\n",
      "fetching 250822 / 13:00 / Lüneburg\n",
      "requesting Uelzen\n",
      "60s pause for ratelimiting...\n",
      "fetching 250822 / 05:00 / Uelzen\n",
      "fetching 250822 / 06:00 / Uelzen\n",
      "fetching 250822 / 07:00 / Uelzen\n",
      "fetching 250822 / 08:00 / Uelzen\n",
      "fetching 250822 / 09:00 / Uelzen\n",
      "fetching 250822 / 10:00 / Uelzen\n",
      "fetching 250822 / 11:00 / Uelzen\n",
      "fetching 250822 / 12:00 / Uelzen\n",
      "fetching 250822 / 13:00 / Uelzen\n",
      "requesting Celle\n",
      "fetching 250822 / 05:00 / Celle\n",
      "fetching 250822 / 06:00 / Celle\n",
      "fetching 250822 / 07:00 / Celle\n",
      "fetching 250822 / 08:00 / Celle\n",
      "fetching 250822 / 09:00 / Celle\n",
      "fetching 250822 / 10:00 / Celle\n",
      "fetching 250822 / 11:00 / Celle\n",
      "fetching 250822 / 12:00 / Celle\n",
      "fetching 250822 / 13:00 / Celle\n",
      "requesting Hannover Hbf\n",
      "fetching 250822 / 05:00 / Hannover Hbf\n",
      "fetching 250822 / 06:00 / Hannover Hbf\n",
      "fetching 250822 / 07:00 / Hannover Hbf\n",
      "fetching 250822 / 08:00 / Hannover Hbf\n",
      "fetching 250822 / 09:00 / Hannover Hbf\n",
      "fetching 250822 / 10:00 / Hannover Hbf\n",
      "fetching 250822 / 11:00 / Hannover Hbf\n",
      "fetching 250822 / 12:00 / Hannover Hbf\n",
      "fetching 250822 / 13:00 / Hannover Hbf\n"
     ]
    }
   ],
   "source": [
    "# request and process timetables\n",
    "import requests\n",
    "import xmltodict\n",
    "import time\n",
    "import os.path as path\n",
    "\n",
    "# load envs\n",
    "db_timetables_base_url = getenv(\"db_timetables_base_url\")\n",
    "db_client_id = getenv(\"db_client_id\")\n",
    "db_client_secret = getenv(\"db_client_secret\")\n",
    "\n",
    "df_stoptimes = None\n",
    "\n",
    "if path.exists(STOPTIMES_PATH):\n",
    "    df_stoptimes = pd.read_csv(STOPTIMES_PATH, dtype=str).dropna(how='all')\n",
    "\n",
    "    # delete entries older than 4 hours\n",
    "\n",
    "    four_hours_ago_tuple = datetimeToDBDateAndHourTuple(datetime.datetime.now() - datetime.timedelta(hours=4))\n",
    "    timestamp_four_hours_ago = f\"{four_hours_ago_tuple[0]}{four_hours_ago_tuple[1]}\"\n",
    "\n",
    "    df_stoptimes = df_stoptimes.drop(df_stoptimes[df_stoptimes['request_timestamp'] < timestamp_four_hours_ago].index)\n",
    "    \n",
    "else:\n",
    "    df_stoptimes = pd.DataFrame(columns=['trip_id', 'category', 'number', 'station_name', 'arrival_dbdatetime', 'departure_dbdatetime', 'request_timestamp','station_uic'])\n",
    "\n",
    "xml_timestamp_tuple = []\n",
    "\n",
    "request_counter = 0\n",
    "\n",
    "\n",
    "for index, station_row in df_stations.iterrows():\n",
    "    station_uic_number = station_row['station_uic']\n",
    "    station_name = station_row['station_name']\n",
    "    \n",
    "    print(f\"requesting {station_name}\")\n",
    "\n",
    "    # respect ratelimiting\n",
    "    if request_counter % 45 == 0 and request_counter > 0:\n",
    "        print(\"60s pause for ratelimiting...\")\n",
    "        time.sleep(60)\n",
    "        \n",
    "    timetable_rows_for_current_station = []\n",
    "        \n",
    "    for date_hour_tuple in date_hour_tuples_to_request:\n",
    "        date_to_request = date_hour_tuple[0] \n",
    "        hour_to_request = date_hour_tuple[1] \n",
    "        request_timestamp = f\"{date_to_request}{hour_to_request}\"\n",
    "        \n",
    "        # check if entry already exists\n",
    "        rows_for_timestamp_and_station = df_stoptimes[(df_stoptimes['request_timestamp'] == request_timestamp) & (df_stoptimes['station_uic'] == station_uic_number)]\n",
    "        no_rows_for_timestamp_and_station = rows_for_timestamp_and_station.shape[0]\n",
    "        \n",
    "        \n",
    "        if no_rows_for_timestamp_and_station > 0:\n",
    "            print(f\"{date_to_request} / {hour_to_request}:00 / {station_name} already present\")\n",
    "            # already fetched\n",
    "            continue\n",
    "        \n",
    "        print(f\"fetching {date_to_request} / {hour_to_request}:00 / {station_name}\")\n",
    "        \n",
    "        # prepare request headers\n",
    "        auth_headers = {\n",
    "            'DB-Client-Id': db_client_id,\n",
    "            'DB-Api-Key': db_client_secret\n",
    "        }\n",
    "        \n",
    "        # {hour:02} means print hour, fill (0) up to 2 (2) digits\n",
    "        url = f\"{db_timetables_base_url}/plan/{station_uic_number}/{date_to_request}/{hour_to_request}\"\n",
    "        \n",
    "        request_counter += 1\n",
    "        \n",
    "        response = requests.get(url=url, headers=auth_headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"skipping hour {hour_to_request}, station {station_uic_number}, {response.status_code}\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        # process response\n",
    "        timetable_dict = xmltodict.parse(response.content)\n",
    "        timetable_stops = timetable_dict['timetable']['s']\n",
    "        \n",
    "        # if there is only one trip in the requested hour, the xml parser parses the timetable stop entry into a dict rather than a list\n",
    "        if not isinstance(timetable_stops, list):\n",
    "            timetable_stops = [timetable_stops]\n",
    "        \n",
    "        for timetable_stop in timetable_stops:\n",
    "            try:\n",
    "                # category (e.g. ICE, RE, S)\n",
    "                trip_label = timetable_stop['tl']\n",
    "                category = trip_label['@c']\n",
    "                number = trip_label['@n']\n",
    "                \n",
    "                # for some idiotic reason, the ids can start with a dash while also being separated by a dash\n",
    "                stop_id = timetable_stop['@id']\n",
    "                \n",
    "                has_dash = True if stop_id.startswith(\"-\") else False\n",
    "\n",
    "                trip_id = None\n",
    "\n",
    "                if has_dash:\n",
    "                    # if stop id has dash, the first split result will be empty\n",
    "                    trip_id = stop_id.split(\"-\")[1]\n",
    "                    trip_id = f\"-{trip_id}\"\n",
    "                else:\n",
    "                    trip_id = stop_id.split(\"-\")[0]\n",
    "                \n",
    "                \n",
    "                # only save Fernverkehr (not using filter flag because NJ dont have them)\n",
    "                if category not in [\"IC\", \"EC\", \"ICE\", \"FLX\", \"WB\", \"RJ\", \"RJX\", \"ECE\", \"EST\", \"TGV\", \"NJ\", \"EN\", \"ES\", \"DN\", \"D\", \"SJ\"]:\n",
    "                    continue\n",
    "                \n",
    "                # arrival\n",
    "                arrival_dbdatetime = None\n",
    "                \n",
    "                if 'ar' in timetable_stop:\n",
    "                    arrival = timetable_stop['ar']\n",
    "                    arrival_dbdatetime = arrival['@pt']\n",
    "                    \n",
    "                # departure\n",
    "                departure_dbdatetime = None\n",
    "                \n",
    "                if 'dp' in timetable_stop:\n",
    "                    departure = timetable_stop['dp']\n",
    "                    departure_dbdatetime = departure['@pt']   \n",
    "                \n",
    "                        \n",
    "                stoptimes_row = pd.DataFrame(data={'trip_id': [trip_id], 'category':[category], 'number': [number], 'station_name':[station_name], 'arrival_dbdatetime': [arrival_dbdatetime], 'departure_dbdatetime':[departure_dbdatetime], 'request_timestamp': [request_timestamp], 'station_uic': station_uic_number})\n",
    "\n",
    "                df_stoptimes = pd.concat([df_stoptimes, stoptimes_row], ignore_index=True)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                \n",
    "    # after every station postprocess and save:\n",
    "    \n",
    "    # remove duplicates\n",
    "    # duplicates can occur when a train arrives before and departs after the full hour, because in this case the train will be included in both slices\n",
    "\n",
    "    df_stoptimes = df_stoptimes.drop_duplicates(subset=['trip_id', 'station_uic', 'departure_dbdatetime'])\n",
    "    \n",
    "    # add departure / arrival where necessary\n",
    "    # set arrival to departure where arrival is na\n",
    "    arrival_isna_mask =  df_stoptimes['arrival_dbdatetime'].isna()\n",
    "    df_stoptimes.loc[arrival_isna_mask, 'arrival_dbdatetime'] = df_stoptimes[arrival_isna_mask]['departure_dbdatetime']\n",
    "\n",
    "    # other way around\n",
    "    departure_isna_mask =  df_stoptimes['departure_dbdatetime'].isna()\n",
    "    df_stoptimes.loc[departure_isna_mask, 'departure_dbdatetime'] = df_stoptimes[departure_isna_mask]['arrival_dbdatetime']\n",
    "    \n",
    "    # sort\n",
    "    df_stoptimes = df_stoptimes.sort_values(by=['trip_id','departure_dbdatetime'])\n",
    "    \n",
    "    # save after every station \n",
    "    df_stoptimes.to_csv(STOPTIMES_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3611706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort dataframe by tripid and departure\n",
    "\n",
    "df_stoptimes = df_stoptimes.sort_values(by=['trip_id','departure_dbdatetime'])\n",
    "\n",
    "df_stoptimes.to_csv(STOPTIMES_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df759122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
