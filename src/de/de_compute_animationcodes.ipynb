{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99c91e9e",
   "metadata": {},
   "source": [
    "# Compute Statuscodes\n",
    "1. check where each train is at the moment\n",
    "2. collect trip ids of trains that are somewhere on the map right now\n",
    "3. check between which stations each trip is\n",
    "4. find the stretch each trip is on\n",
    "5. calculate the position of each train on their respective stretch\n",
    "6. lookup the stretch segment each stretch is on\n",
    "7. put the statuscode together and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "75b146c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250810\n",
      "2508101222\n",
      "2025-08-10\n",
      "2025-08-10 12:22:00\n",
      "('250810', '12')\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "STOPTIMES_PATH = 'stoptimes.csv'\n",
    "STRETCHES_PATH = './static/stretches.csv'\n",
    "STRETCH_SEGMENTS_PATH = './static/stretch_segments.csv'\n",
    "STATIONS_PATH = './static/stations.csv'\n",
    "FOCUS_SIGNATURES_PATH = './static/focus_signatures.csv'\n",
    "TRIPID_STRETCHID_PATH = './tripid_stretchid.csv'\n",
    "\n",
    "# chatgpt generiert lol\n",
    "import datetime\n",
    "def dateToDBDate(date: datetime.date) -> str:\n",
    "    \"\"\"\n",
    "    Wandelt ein datetime.date Objekt in einen DB-Date-String \"YYMMDD\" um.\n",
    "    \"\"\"\n",
    "    return date.strftime(\"%y%m%d\")\n",
    "\n",
    "\n",
    "def datetimeToDBDatetime(dt: datetime.datetime) -> str:\n",
    "    \"\"\"\n",
    "    Wandelt ein datetime.datetime Objekt in einen DB-Datetime-String \"YYMMDDHHMM\" um.\n",
    "    \"\"\"\n",
    "    return dt.strftime(\"%y%m%d%H%M\")\n",
    "\n",
    "\n",
    "def DBDatetimeToDatetime(dbDate: str) -> datetime.datetime:\n",
    "    \"\"\"\n",
    "    Wandelt einen DB-Datetime-String \"YYMMDDHHMM\" in ein datetime.datetime Objekt um.\n",
    "    \"\"\"\n",
    "    return datetime.datetime.strptime(dbDate, \"%y%m%d%H%M\")\n",
    "\n",
    "\n",
    "def DBDateToDate(dbDate: str) -> datetime.date:\n",
    "    \"\"\"\n",
    "    Wandelt einen DB-Date-String \"YYMMDD\" in ein datetime.date Objekt um.\n",
    "    \"\"\"\n",
    "    return datetime.datetime.strptime(dbDate, \"%y%m%d\").date()\n",
    "\n",
    "def datetimeToDBDateAndHourTuple(dt: datetime.datetime):\n",
    "    date = dt.strftime(\"%y%m%d\")\n",
    "    hour = dt.strftime(\"%H\")\n",
    "    return (date, hour)\n",
    "    \n",
    "       \n",
    "print(dateToDBDate(datetime.date(2025, 8, 10)))\n",
    "print(datetimeToDBDatetime(datetime.datetime(2025, 8, 10, 12, 22)))\n",
    "print(DBDateToDate(\"250810\"))\n",
    "print(DBDatetimeToDatetime(\"2508101222\"))\n",
    "print(datetimeToDBDateAndHourTuple(datetime.datetime(2025, 8, 10, 12, 22)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "06a0a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stoptimes = pd.read_csv(STOPTIMES_PATH, dtype=\"string\", parse_dates=['arrival', \"departure\"]).dropna(how='all')\n",
    "\n",
    "# remove request_timestamp and request_uic because they were only needed in the df_stoptimes.csv for the preprocessing\n",
    "# to identify outdated and cached data and during change application to match the changes to the stoptime entries.\n",
    "# But they are not needed anymore and as the stoptimes.csv is only read but not overwritten in this script, they can go\n",
    "df_stoptimes = df_stoptimes.drop(labels=['request_timestamp'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "3afddccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 111 active trips\n"
     ]
    }
   ],
   "source": [
    "# find active trip ids\n",
    "\n",
    "current_datetime = datetime.datetime.now()\n",
    "    \n",
    "# train is departed, when the departure time was before now\n",
    "df_stoptimes['has_departed_station'] = df_stoptimes['departure'] < current_datetime\n",
    "\n",
    "# find tripids with one stop time and remove them, as for identifying a position on a stretch at least two stops are needed.\n",
    "tripids_with_one_stoptime_mask = df_stoptimes.groupby(\"trip_id\")[\"trip_id\"].transform(\"count\") == 1\n",
    "df_stoptimes = df_stoptimes.drop(df_stoptimes[tripids_with_one_stoptime_mask].index)\n",
    "\n",
    "# find tripids where all stops have been departed and remove them, as that means the trip has ended\n",
    "tripids_ended_mask = df_stoptimes.groupby(\"trip_id\")[\"has_departed_station\"].transform(\"all\")\n",
    "df_stoptimes = df_stoptimes.drop(df_stoptimes[tripids_ended_mask].index)\n",
    "\n",
    "# find tripids where not any stops have been departed and remove them, as that means the trip has not started yet\n",
    "tripids_not_started_mask = ~df_stoptimes.groupby(\"trip_id\")[\"has_departed_station\"].transform(\"any\")\n",
    "df_stoptimes = df_stoptimes.drop(df_stoptimes[tripids_not_started_mask].index)\n",
    "\n",
    "# only the active trips remain\n",
    "\n",
    "no_active_trips = df_stoptimes['trip_id'].unique().shape[0]\n",
    "print(f\"found {no_active_trips} active trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "455fb184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the two stations, between which each train is traveling (standing at a station until departure counts to being between the two stations. \n",
    "# The stopping time animation is handled by calculating where the train is between the two stations and \n",
    "# standing at the second station simply gets mapped to having travelled 100% of the time)\n",
    "# therefore the previous station is the last of the departed stations and the next station is the first of the undeparted stations\n",
    "\n",
    "previous_stations = df_stoptimes[df_stoptimes['has_departed_station'] == True].groupby(by=['trip_id'], as_index=False).last()\n",
    "next_stations = df_stoptimes[df_stoptimes['has_departed_station'] == False].groupby(by=['trip_id'], as_index=False).first()\n",
    "\n",
    "# has_departed_station now not needed anymore\n",
    "previous_stations = previous_stations.drop(labels=['has_departed_station'], axis=1)\n",
    "next_stations = next_stations.drop(labels=['has_departed_station'], axis=1)\n",
    "\n",
    "df_trip_statuses = pd.merge(how='inner', left=previous_stations, right=next_stations, on=['trip_id', 'category', 'number'], suffixes=(\"_previous\", \"_next\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "244fe194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load graph representation of network\n",
    "# station -> nodes\n",
    "# stretches -> edges\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "stretches = pd.read_csv(STRETCHES_PATH, dtype=str).dropna(how='all')\n",
    "stations = pd.read_csv(STATIONS_PATH, dtype=str).dropna(how='all')\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "# add nodes\n",
    "station_list = list(stations.itertuples(index=False, name=None)) # [(station_name, station_uic)]\n",
    "nodes = [(station[1], {'station_name': station[0]}) for station in station_list]\n",
    "G.add_nodes_from(nodes)\n",
    "\n",
    "# add edges\n",
    "# attraction force is used to draw the graph by defining the attraction force between the two nodes and is the inverted travel time\n",
    "edge_list = list(stretches[['station_uic_from', 'station_uic_to', 'station_name_from','station_name_to','travel_cost','super_name']].itertuples(index=False, name=None)) # [(station_name_from, station_uic_from, station_name_to, station_uic_from, travel_cost, super_name)]\n",
    "edges = [(edge[0], edge[1], {'station_name_from': edge[2], 'station_name_to': edge[3], 'travel_cost': int(edge[4]), 'super_name': edge[5]}) for edge in edge_list]\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "2ecf0366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate where the train is\n",
    "import numpy as np\n",
    "\n",
    "# calculate path between the stations, the trip is currently travelling\n",
    "def compute_position_on_graph(row):\n",
    "    previous_station = row['station_uic_previous']\n",
    "    next_station = row['station_uic_next']\n",
    "    previous_station_departure:datetime.datetime = row['departure_previous']\n",
    "    next_station_arrival:datetime.datetime = row['arrival_next']\n",
    "    \n",
    "    shortest_path = nx.shortest_path(G, previous_station, next_station, weight='travel_cost')\n",
    "    \n",
    "    # calculate total travel cost for shortest path\n",
    "    edges = []\n",
    "    total_travel_cost = 0.0\n",
    "    for u, v in zip(shortest_path[:-1], shortest_path[1:]):\n",
    "        travel_cost = int(G[u][v]['travel_cost'])\n",
    "        total_travel_cost += travel_cost\n",
    "        edges.append([u,v,travel_cost])\n",
    "    \n",
    "    # calculate relative travel cost for edge relative to the total cost of the whole path\n",
    "    edges = np.array(edges, dtype=str)\n",
    "    relative_travel_costs = [int(edge[2]) / total_travel_cost for edge in edges]\n",
    "\n",
    "    # cumulative travel costs add up to one, as they were normalized before in relation to total travel cost.\n",
    "    # therefore, the cumulative_relative_travel_costs map to how much progress a trip needs to make on the shortest path to finish a edge of the path.\n",
    "    cumulative_relative_travel_costs = np.cumsum(relative_travel_costs)\n",
    "    \n",
    "    # calculate trip progress on the shortest path\n",
    "    current_time = datetime.datetime.now()\n",
    "    time_elapsed_since_last_stop = current_time - previous_station_departure\n",
    "    trip_travel_time = next_station_arrival - previous_station_departure\n",
    "    trip_progress = time_elapsed_since_last_stop.total_seconds() / trip_travel_time.total_seconds()\n",
    "    \n",
    "    # find the first edge that has not been finished which is the edge currently travelled on\n",
    "    # done by calculating binary array, which returns false for finished (progress < cumcost) and true for finished (progress > cumcost) segments.\n",
    "    # argmax returns the first occurence of the maximum value and 0 in case of no maximum value which is convenient, as the 0th edge is being travelled if\n",
    "    # none are finished and therefore the operation returns in a list full of False \n",
    "    index_of_current_edge = np.argmax(trip_progress < cumulative_relative_travel_costs)\n",
    "    \n",
    "    # with the index, the current edge can be retrieved from the edges\n",
    "    current_edge = edges[index_of_current_edge]\n",
    "    \n",
    "    # to calculate the relative progress on the current edge, the total progress - the relative cost of all finished edges will be divided by the relative cost of the current edge \n",
    "    current_edge_relative_travel_cost = relative_travel_costs[index_of_current_edge]\n",
    "    current_edge_progress_upper_bound = cumulative_relative_travel_costs[index_of_current_edge]\n",
    "    current_edge_progress_lower_bound = current_edge_progress_upper_bound - current_edge_relative_travel_cost\n",
    "    \n",
    "    progress_on_current_edge = (trip_progress - current_edge_progress_lower_bound) / current_edge_relative_travel_cost\n",
    "    \n",
    "    # if the train is standing in the station, progress will show more than 100%, because the time since last departure is longer than \n",
    "    # the time between last departure and next arrival\n",
    "    percentage_on_current_edge = min(int(progress_on_current_edge * 100), 100)\n",
    "    \n",
    "    station_uic_from = current_edge[0]\n",
    "    station_uic_to = current_edge[1]\n",
    "    # find id of current edge in stretches\n",
    "    \n",
    "    travels_in_reverse_direction = False\n",
    "    \n",
    "    current_stretch = stretches[(stretches['station_uic_from'] == station_uic_from) & (stretches['station_uic_to'] == station_uic_to)]\n",
    "    \n",
    "    # stretches are defined one way but work both ways\n",
    "    if len(current_stretch) == 0:\n",
    "        current_stretch = stretches[(stretches['station_uic_to'] == station_uic_from) & (stretches['station_uic_from'] == station_uic_to)]\n",
    "        travels_in_reverse_direction = True\n",
    "    \n",
    "    if len(current_stretch) == 0:\n",
    "        print(f'No Stretch found for edge: {current_edge}')\n",
    "        \n",
    "    current_stretch_id = current_stretch.iloc[0]['stretch_id']\n",
    "    current_stretch_name = current_stretch.iloc[0]['stretch_name']\n",
    "    current_stretch_super_name = current_stretch.iloc[0]['super_name']\n",
    "    \n",
    "    return {'stretch_super_name': current_stretch_super_name,'stretch_name': current_stretch_name,'progress': percentage_on_current_edge, 'stretch_id': current_stretch_id,  'travelling_reverse': travels_in_reverse_direction}\n",
    "    \n",
    "df_trip_statuses['position'] = df_trip_statuses.apply(compute_position_on_graph, axis=1) \n",
    "\n",
    "# arrival and departure times and stations are no longer needed\n",
    "\n",
    "#df_trip_statuses = df_trip_statuses.drop(['station_name_previous','station_uic_previous','arrival_previous','departure_previous','station_name_next','station_uic_next','arrival_next','departure_next'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "f12fb151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup row in stretch_id + % to stretch_segment / LED mapping\n",
    "from collections import defaultdict\n",
    "\n",
    "types = defaultdict(lambda: str, lower_bound = \"int\", upper_bound = \"int\")\n",
    "stretch_segments = pd.read_csv(STRETCH_SEGMENTS_PATH, dtype=types).dropna(how='all')\n",
    "\n",
    "# find rows where the progress of the trip fits, can be 1 or 2 if the progress is right on the edge because of both sides inclusive\n",
    "def compute_primary_statuscode(row):\n",
    "    position = row['position']\n",
    "    stretch_id = position['stretch_id']\n",
    "    progress = int(position['progress'])\n",
    "    is_travelling_reverse = position['travelling_reverse']\n",
    "    \n",
    "    # if the train is travelling in the inverse direction, the progress needs to be inverted. Hamburg -> Hannover 20% and Hannover -> Hamburg 80% are the same positions\n",
    "    stretch_position = (100 - progress) if is_travelling_reverse else progress\n",
    "    \n",
    "    # find stretch segments that fit and take the first one, as it is right on the edge between two segments if there are more than one\n",
    "    stretch_id_mask = stretch_segments['stretch_id'] == stretch_id\n",
    "    lower_bound_mask = stretch_segments['lower_bound'] <= stretch_position\n",
    "    upper_bound_mask = stretch_position <= stretch_segments['upper_bound']\n",
    "    \n",
    "    applicable_segments = stretch_segments[stretch_id_mask & lower_bound_mask & upper_bound_mask]\n",
    "    \n",
    "    segment = None\n",
    "    if len(applicable_segments) < 1:\n",
    "        return\n",
    "    else:\n",
    "        segment = applicable_segments.iloc[0]\n",
    "    \n",
    "    statuscode = f\"{segment['stretch_id']}_{segment['segment_number']}\"\n",
    "    \n",
    "    return statuscode\n",
    "\n",
    "# optionally compute statuscode for secondary LED by building list of all segment rows along the path, then group by LED and find the LED before the current one.\n",
    "\n",
    "df_trip_statuses['primary_statuscode'] = df_trip_statuses.apply(compute_primary_statuscode, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "49a93f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute delay\n",
    "# choose arrival_delay_next if the train has not reached the next station, choose departure_delay_next if the train has reached the next station\n",
    "\n",
    "def compute_delay(row):\n",
    "    arrival_next = row['arrival_next']\n",
    "    \n",
    "    # train is still travelling    \n",
    "    if arrival_next > datetime.datetime.now():\n",
    "        return int(row['arrival_delay_next'])\n",
    "    \n",
    "    # train has reached next station\n",
    "    else:\n",
    "        return int(row['departure_delay_next'])     \n",
    "    \n",
    "df_trip_statuses['delay'] = df_trip_statuses.apply(compute_delay, axis=1)\n",
    "\n",
    "# if the memory is needed, drop the old delay columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "dbee0f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add category priority (higher number = higher priority)\n",
    "def compute_category_priority(category: str):\n",
    "\n",
    "    if category in ['ICE']:\n",
    "        return 0\n",
    "    \n",
    "    if category in ['IC']:\n",
    "        return 1\n",
    "\n",
    "    if category in ['EC','FLX','WB', 'RJ', 'RJX', 'ECE', 'EST','TGV', 'NJ','EN','ES','DN','D','SJ']:\n",
    "        return 2\n",
    "    \n",
    "df_trip_statuses['category_priority'] = df_trip_statuses['category'].map(compute_category_priority)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9aeca9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (Python 3.11.2)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/robin/Documents/github/heidelberg-train-monitor/venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# color mapping for animationcodes\n",
    "\n",
    "def get_color_for_category(category: str):\n",
    "    if category in ['IC']:\n",
    "        return \"008F8F\"\n",
    "    \n",
    "    if category in ['EC']:\n",
    "        return \"0000AA\"\n",
    "    \n",
    "    if category in ['ICE']:\n",
    "        return \"ADA869\"\n",
    "    \n",
    "    if category in ['FLX','WB']:\n",
    "        return \"008F00\"\n",
    "    \n",
    "    if category in ['RJ', 'RJX', 'ECE']:\n",
    "        return \"A96000\"\n",
    "    \n",
    "    if category in ['EST','TGV']:\n",
    "        return \"A00000\"\n",
    "    \n",
    "    if category in ['NJ','EN','ES','DN','D','SJ']:\n",
    "        return \"8900AC\"\n",
    "    \n",
    "    return \"FFFF00\"\n",
    "\n",
    "\n",
    "def get_fullbright_color_for_category(category: str):\n",
    "    if category in ['IC']:\n",
    "        return \"00FFFF\"\n",
    "    \n",
    "    if category in ['EC']:\n",
    "        return \"0000FF\"\n",
    "    \n",
    "    if category in ['ICE']:\n",
    "        return \"FFF99E\"\n",
    "    \n",
    "    if category in ['FLX','WB']:\n",
    "        return \"00FF00\"\n",
    "    \n",
    "    if category in ['RJ', 'RJX', 'ECE']:\n",
    "        return \"FF9100\"\n",
    "    \n",
    "    if category in ['EST','TGV']:\n",
    "        return \"FF0000\"\n",
    "    \n",
    "    if category in ['NJ','EN','ES','DN','D','SJ']:\n",
    "        return \"CC00FF\"\n",
    "    \n",
    "    return \"FFFF00\"\n",
    "\n",
    "def get_color_for_delay(delay: str):\n",
    "    if delay > 45:\n",
    "        return \"8F0000\"\n",
    "    if delay > 15:\n",
    "        return \"877100\"\n",
    "    else:\n",
    "        return \"328F00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "7273056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build animationcodes\n",
    "\n",
    "def compute_primary_animationcodes(row, color_by: str):\n",
    "    if color_by == 'delay':\n",
    "        color = get_color_for_delay(row['delay'])\n",
    "    else:\n",
    "        color = get_color_for_category(row['category'])\n",
    "        \n",
    "    statuscode = row['primary_statuscode']\n",
    "    return f\"DE:{statuscode}:{color}\"\n",
    "\n",
    "# sort by category priority. Later statuscodes override earlier ones, so ascending. Then compute animationcodes\n",
    "primary_category_animationcodes = pd.DataFrame(data={'animationcode': df_trip_statuses.sort_values(by=['category_priority']).apply(compute_primary_animationcodes, axis=1, args=('category',))})\n",
    "\n",
    "# sort by delay. Later statuscodes override earlier ones, so ascending. Then compute animationcodes\n",
    "primary_delay_animationcodes = pd.DataFrame(data={'animationcode': df_trip_statuses.sort_values(by=['delay']).apply(compute_primary_animationcodes, axis=1, args=('delay',))})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a16222",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_category_animationcodes.to_csv('de_category_animationcodes.csv', index=False)\n",
    "primary_delay_animationcodes.to_csv('de_delay_animationcodes.csv', index=False)\n",
    "\n",
    "df_trip_statuses.to_csv('temp.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a22e128",
   "metadata": {},
   "source": [
    "### Focus animationscodes\n",
    "\n",
    "if the focus signature train is running, pick it. Do that until either all focus signatures have been looked up or a maximum of 4 focus trips is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "e68870e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_hex_color(hex_color, factor):\n",
    "    # HEX -> RGB\n",
    "    r = int(hex_color[0:2], 16)\n",
    "    g = int(hex_color[2:4], 16)\n",
    "    b = int(hex_color[4:6], 16)\n",
    "\n",
    "    # Dimmen und sicherstellen, dass der Wert im Bereich [0, 255] bleibt\n",
    "    r = int(max(0, min(255, r * factor)))\n",
    "    g = int(max(0, min(255, g * factor)))\n",
    "    b = int(max(0, min(255, b * factor)))\n",
    "\n",
    "    # RGB -> HEX\n",
    "    return \"{:02X}{:02X}{:02X}\".format(r, g, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c7485d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tripid_stretchid.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[239]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m df_focus_signatures = pd.read_csv(FOCUS_SIGNATURES_PATH, dtype=\u001b[38;5;28mstr\u001b[39m).dropna(how=\u001b[33m'\u001b[39m\u001b[33mall\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df_tripid_stretchid = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRIPID_STRETCHID_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m.dropna(how=\u001b[33m'\u001b[39m\u001b[33mall\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m focus_trips_found = \u001b[32m0\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# remove all old focus trips, in case there were 4 found and now only 3, so that the fourth one doesnt go stale\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\robin\\Documents\\github\\heidelberg-train-monitor\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\robin\\Documents\\github\\heidelberg-train-monitor\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\robin\\Documents\\github\\heidelberg-train-monitor\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\robin\\Documents\\github\\heidelberg-train-monitor\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\robin\\Documents\\github\\heidelberg-train-monitor\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m    876\u001b[39m             encoding=ioargs.encoding,\n\u001b[32m    877\u001b[39m             errors=errors,\n\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './tripid_stretchid.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "df_focus_signatures = pd.read_csv(FOCUS_SIGNATURES_PATH, dtype=str).dropna(how='all')\n",
    "df_tripid_stretchid = pd.read_csv(TRIPID_STRETCHID_PATH, dtype=str).dropna(how='all')\n",
    "\n",
    "focus_trips_found = 0\n",
    "\n",
    "# remove all old focus trips, in case there were 4 found and now only 3, so that the fourth one doesnt go stale\n",
    "\n",
    "for i in range(6):\n",
    "    filename = f\"de_focus_animationcodes_{i}.csv\"\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "        print(f\"{filename} wurde gelöscht.\")\n",
    "    else:\n",
    "        print(f\"{filename} existiert nicht.\")\n",
    "\n",
    "\n",
    "\n",
    "for index, focus_signature_row in df_focus_signatures.iterrows():\n",
    "    \n",
    "    if focus_trips_found >= 6:\n",
    "        break\n",
    "    \n",
    "    category = focus_signature_row['category']\n",
    "    number = focus_signature_row['number']\n",
    "    \n",
    "    \n",
    "    focus_trip_candidate = df_trip_statuses[(df_trip_statuses['category'] == category) & (df_trip_statuses['number'] == number)]\n",
    "    \n",
    "    if len(focus_trip_candidate) == 0:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    \n",
    "    focus_trip = focus_trip_candidate.iloc[0]\n",
    "    \n",
    "    color_position = get_fullbright_color_for_category(focus_trip['category'])\n",
    "    color_way_highlighting = dim_hex_color(color_position, 0.4)\n",
    "    \n",
    "    focus_animationcodes = pd.DataFrame(columns=['animationcode'])\n",
    "    \n",
    "    stretchids_for_this_trip = df_tripid_stretchid[df_tripid_stretchid['trip_id'] == focus_trip['trip_id']]\n",
    "    \n",
    "    stretch_segments_for_this_trip = pd.merge(right=stretchids_for_this_trip, left=stretch_segments, on=['stretch_id'])\n",
    "    \n",
    "    stretch_segments_for_this_trip['primary_statuscode'] = stretch_segments_for_this_trip.apply(lambda row: f\"{row['stretch_id']}_{row['segment_number']}\", axis=1)\n",
    "    focus_animationcodes['animationcode'] = stretch_segments_for_this_trip['primary_statuscode'].map(lambda statuscode: f\"DE:{statuscode}:{color_way_highlighting}\")\n",
    "\n",
    "    position_animationcode_row = pd.DataFrame({'animationcode': [f\"DE:{focus_trip['primary_statuscode']}:{color_position}\"]})\n",
    "    focus_animationcodes = pd.concat([focus_animationcodes, position_animationcode_row])\n",
    "    \n",
    "    focus_animationcodes.to_csv(f'./de_focus_animationcodes_{focus_trips_found}.csv', index=False)\n",
    "    focus_trips_found += 1 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
